{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale-py in c:\\users\\alex\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\alex\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ale-py) (1.24.2)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\alex\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ale-py) (5.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alex\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ale-py) (4.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\Alex\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\alex\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\Alex\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install ale-py\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Load ROMS\n",
    "\n",
    "Get a rom from the bellow link, extract it and then run the `ale-import-roms .` command in the forlder with the roms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supported Games: https://github.com/mgbellemare/Arcade-Learning-Environment/blob/master/docs/games.md\n",
    "from ale_py.roms import Phoenix\n",
    "from ale_py import ALEInterface, SDL_SUPPORT\n",
    "\n",
    "ale = ALEInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(SDL_SUPPORT)\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get & Set the desired settings\n",
    "ale.setInt(\"random_seed\", 123)\n",
    "# The default is already 0.25, this is just an example\n",
    "ale.setFloat(\"repeat_action_probability\", 0.25)\n",
    "\n",
    "# Check if we can display the screen\n",
    "# For the first set of training better let it without UI/sound\n",
    "if SDL_SUPPORT:\n",
    "    ale.setBool(\"sound\", True)\n",
    "    ale.setBool(\"display_screen\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our game\n",
    "ale.loadROM(Phoenix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEpsilonGreedyPolicy(Q, epsilon, num_actions):\n",
    "\t\"\"\"\n",
    "\tCreates an epsilon-greedy policy based\n",
    "\ton a given Q-function and epsilon.\n",
    "\t\n",
    "\tReturns a function that takes the state\n",
    "\tas an input and returns the probabilities\n",
    "\tfor each action in the form of a numpy array\n",
    "\tof length of the action space(set of possible actions).\n",
    "\t\"\"\"\n",
    "\tdef policyFunction(state):\n",
    "\n",
    "\t\tAction_probabilities = np.ones(num_actions,\n",
    "\t\t\t\tdtype = float) * epsilon / num_actions\n",
    "\t\t\t\t\n",
    "\t\tbest_action = np.argmax(Q[state])\n",
    "\t\tAction_probabilities[best_action] += (1.0 - epsilon)\n",
    "\t\treturn Action_probabilities\n",
    "\n",
    "\treturn policyFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromArrayToHash(x):\n",
    "    return hash(x.tostring())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the player's coordinates\n",
    "def get_player_position(s):\n",
    "    m,n = s.shape\n",
    "    player_x = None\n",
    "    player_y = None\n",
    "    for i in range(0,m):\n",
    "        for j in range (0,n):\n",
    "            if s[i][j] == 56 and s[i+4][j] == 56:\n",
    "                player_x = i\n",
    "                player_y = j\n",
    "                return player_x,player_y\n",
    "    return player_x,player_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to get the closest enemy's coordinates\n",
    "    based on the player's position\n",
    "\"\"\"\n",
    "def get_enemy_position(s,player_x,player_y):\n",
    "    m,n = s.shape\n",
    "    enemy_x = None\n",
    "    enemy_y = None\n",
    "    for i in range(player_x, m):\n",
    "        for j in range(player_y, n):\n",
    "            if s[i][j] == 100:\n",
    "                enemy_x = i\n",
    "                enemy_y = j\n",
    "                return enemy_x, enemy_y\n",
    "    for i in range(player_x, 0):\n",
    "        for j in range(player_y, n):\n",
    "            if s[i][j] == 100:\n",
    "                enemy_x = i\n",
    "                enemy_y = j\n",
    "                return enemy_x, enemy_y\n",
    "    return enemy_x,enemy_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(env, num_episodes, discount_factor = 1.0,\n",
    "\t\t\t\t\t\t\talpha = 0.6, epsilon = 0.1):\n",
    "\n",
    "\t\"\"\"\n",
    "\tQ-Learning algorithm: Off-policy TD control.\n",
    "\tFinds the optimal greedy policy while improving\n",
    "\tfollowing an epsilon-greedy policy\"\"\"\n",
    "\n",
    "\t# Action value function\n",
    "\t# A nested dictionary that maps\n",
    "\t# state -> (action -> action-value).\n",
    "\n",
    "\tall_actions = env.getLegalActionSet()\n",
    "\tlegal_actions = all_actions[0:1] + all_actions[3:4]\n",
    "\tnum_actions = len(legal_actions)\n",
    "\tprint(num_actions)\n",
    "\tQ = defaultdict(lambda: np.zeros(num_actions))\t\n",
    "\n",
    "\t# Create an epsilon greedy policy function\n",
    "\t# appropriately for environment action space\n",
    "\tpolicy = createEpsilonGreedyPolicy(Q, epsilon, num_actions)\n",
    "\t\n",
    "\t# For every episode\n",
    "\tfor _ in range(num_episodes):\n",
    "\t\t\n",
    "\t\t# Reset the environment and pick the first action\n",
    "\t\tenv.reset_game()\n",
    "\t\t\n",
    "\t\tscreen = env.getScreen()\n",
    "\t\tx, y = get_player_position(screen)\n",
    "\t\tif(x != None and y != None):\n",
    "\t\t\t\te_x, e_y = get_enemy_position(screen, x, y)\n",
    "\t\t\t\tstate = (x, y, e_x, e_y) \n",
    "\t\telse: action = (0)\t\t\n",
    "\n",
    "\t\tfor t in itertools.count():\n",
    "\t\t\t\n",
    "\t\t\t# get probabilities of all actions from current state\n",
    "\t\t\taction_probabilities = policy(state)\n",
    "\n",
    "\t\t\t# choose action according to\n",
    "\t\t\t# the probability distribution\n",
    "\t\t\taction = np.random.choice(np.arange(\n",
    "\t\t\t\t\tlen(action_probabilities)),\n",
    "\t\t\t\t\tp = action_probabilities)\n",
    "\n",
    "\t\t\t# take action and get reward, transit to next state\n",
    "\t\t\treward = env.act(action)\n",
    "\t\t\tdone = env.game_over()\n",
    "\t\t\t\n",
    "\t\t\tscreen = env.getScreen()\n",
    "\t\t\tx, y = get_player_position(screen)\n",
    "\t\t\tif(x != None and y != None):\n",
    "\t\t\t\te_x, e_y = get_enemy_position(screen, x, y)\n",
    "\t\t\t\tnext_state = (x, y, e_x, e_y) \n",
    "\t\t\telse: action = (0)\n",
    "\t\t\t\n",
    "\t\t\t# TD Update\n",
    "\t\t\tbest_next_action = np.argmax(Q[next_state])\t\n",
    "\t\t\ttd_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "\t\t\ttd_delta = td_target - Q[state][action]\n",
    "\t\t\tQ[state][action] += alpha * td_delta\n",
    "\t\t\t\n",
    "\t\t\t# done is True if episode terminated\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tstate = next_state\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\treturn Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available modes: 1\n",
      "Number of available difficulties: 1\n",
      "2\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "avail_modes = ale.getAvailableModes()\n",
    "avail_diff = ale.getAvailableDifficulties()\n",
    "\n",
    "print(f\"Number of available modes: {len(avail_modes)}\")\n",
    "print(f\"Number of available difficulties: {len(avail_diff)}\")\n",
    "\n",
    "# Get the list of legal actions\n",
    "legal_actions = ale.getLegalActionSet()\n",
    "\n",
    "ale.setDifficulty(avail_diff[0])\n",
    "ale.setMode(avail_modes[0])\n",
    "ale.reset_game()\n",
    "state=qLearning(ale, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5204562b952d1c4aff549ff4e436a4d4da975b74dcc1553092782cc0bb4e7dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
